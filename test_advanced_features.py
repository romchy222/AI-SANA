#!/usr/bin/env python3
"""
Comprehensive Test Suite for Advanced AI Agent Features
ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²

This script tests all the new ML-powered features:
1. ML-based Intent Classification
2. Analytics and Learning System
3. User Personalization
4. Semantic Search with Knowledge Graphs
5. Distributed Caching and Async Processing
"""

import sys
import os
import time
import random
from typing import Dict, List, Any

# Add the parent directory to sys.path to import modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_intent_classification():
    """Test ML-based intent classification"""
    print("ğŸ§  Testing ML-based Intent Classification...")
    
    try:
        from intent_classifier import intent_classifier
        
        test_queries = [
            ("ĞºĞ°Ğº Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚", "ai_abitur"),
            ("Ğ½ÑƒĞ¶Ğ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ñ", "ai_abitur"),
            ("Ğ³Ğ´Ğµ Ğ²Ğ·ÑÑ‚ÑŒ ÑĞ¿Ñ€Ğ°Ğ²ĞºÑƒ Ğ¾Ğ± Ğ¾Ñ‚Ğ¿ÑƒÑĞºĞµ", "kadrai"),
            ("ĞºĞ¾Ğ³Ğ´Ğ° Ğ±ÑƒĞ´ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ¾Ğ²", "uninav"),
            ("Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ¸Ñ‚Ğµ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ", "career_navigator"),
            ("Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ·Ğ°ÑĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¶Ğ¸Ñ‚Ğ¸Ğµ", "uniroom")
        ]
        
        correct_predictions = 0
        total_tests = len(test_queries)
        
        for query, expected_agent in test_queries:
            scores = intent_classifier.classify_intent(query, 'ru')
            predicted_agent = max(scores, key=scores.get) if scores else None
            
            if predicted_agent == expected_agent:
                correct_predictions += 1
                status = "âœ…"
            else:
                status = "âŒ"
            
            print(f"  {status} Query: '{query}'")
            print(f"     Expected: {expected_agent}, Predicted: {predicted_agent}")
            print(f"     Scores: {dict(list(scores.items())[:3])}")
        
        accuracy = correct_predictions / total_tests * 100
        print(f"\nğŸ“Š Intent Classification Accuracy: {accuracy:.1f}% ({correct_predictions}/{total_tests})")
        
        # Test learning from feedback
        intent_classifier.learn_from_feedback(
            "ĞºĞ°Ğº Ğ¿Ğ¾Ğ´Ğ°Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğµ", 
            "ai_abitur", 
            0.9, 
            'ru'
        )
        print("âœ… Feedback learning tested successfully")
        
        # Get learning statistics
        stats = intent_classifier.get_learning_stats()
        print(f"ğŸ“ˆ Learning Stats: {stats}")
        
        return accuracy > 70  # Consider 70%+ accuracy as success
        
    except Exception as e:
        print(f"âŒ Intent Classification Test Failed: {e}")
        return False


def test_analytics_engine():
    """Test comprehensive analytics and A/B testing"""
    print("\nğŸ“Š Testing Analytics and Learning System...")
    
    try:
        from analytics_engine import analytics_engine
        
        # Test interaction tracking
        test_interactions = [
            {
                'user_id': 'test_user_1',
                'message': 'ĞšĞ°Ğº Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚?',
                'agent_type': 'ai_abitur',
                'agent_name': 'AI-Abitur',
                'confidence': 0.85,
                'response_time': 1.2,
                'cached': False,
                'context_used': True,
                'context_confidence': 0.7,
                'language': 'ru'
            },
            {
                'user_id': 'test_user_2',
                'message': 'Ğ Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ¾Ğ²',
                'agent_type': 'uninav',
                'agent_name': 'UniNav',
                'confidence': 0.9,
                'response_time': 0.8,
                'cached': True,
                'context_used': True,
                'context_confidence': 0.8,
                'language': 'ru',
                'user_rating': 0.9
            }
        ]
        
        # Track test interactions
        for interaction in test_interactions:
            analytics_engine.track_interaction(interaction)
        
        print("âœ… Interaction tracking tested")
        
        # Test performance metrics
        metrics = analytics_engine.get_performance_metrics(time_window_hours=1)
        print(f"ğŸ“ˆ Performance Metrics: {metrics}")
        
        # Test A/B testing framework
        test_created = analytics_engine.create_ab_test(
            "response_style_test",
            ["detailed", "concise"],
            {"detailed": 0.5, "concise": 0.5}
        )
        
        if test_created:
            # Test variant assignment
            variant1 = analytics_engine.assign_ab_test_variant("response_style_test", "user1")
            variant2 = analytics_engine.assign_ab_test_variant("response_style_test", "user2")
            print(f"âœ… A/B Testing: user1 -> {variant1}, user2 -> {variant2}")
            
            # Track some test results
            analytics_engine.track_ab_test_result("response_style_test", variant1, "satisfaction", 0.8)
            analytics_engine.track_ab_test_result("response_style_test", variant2, "satisfaction", 0.75)
            
            # Get test results
            ab_results = analytics_engine.get_ab_test_results("response_style_test")
            print(f"ğŸ“Š A/B Test Results: {ab_results}")
        
        # Test error tracking
        analytics_engine.track_error({
            'error_type': 'test_error',
            'agent_type': 'ai_abitur',
            'message': 'test message',
            'error_details': 'simulated error for testing',
            'user_impact': 'minor'
        })
        print("âœ… Error tracking tested")
        
        # Generate insights report
        insights = analytics_engine.generate_insights_report()
        print(f"ğŸ” Insights Report Generated: {len(insights)} sections")
        
        return True
        
    except Exception as e:
        print(f"âŒ Analytics Engine Test Failed: {e}")
        return False


def test_personalization_engine():
    """Test user personalization and learning"""
    print("\nğŸ‘¤ Testing Personalization and Learning...")
    
    try:
        from personalization_engine import personalization_engine
        
        test_user = "test_user_123"
        
        # Simulate user interactions
        interactions = [
            {
                'message': 'ĞšĞ°Ğº Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚?',
                'agent_type': 'ai_abitur',
                'agent_name': 'AI-Abitur',
                'confidence': 0.85,
                'response_time': 1.2,
                'language': 'ru'
            },
            {
                'message': 'ĞšĞ°ĞºĞ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½ÑƒĞ¶Ğ½Ñ‹?',
                'agent_type': 'ai_abitur',
                'agent_name': 'AI-Abitur',
                'confidence': 0.9,
                'response_time': 1.0,
                'language': 'ru'
            },
            {
                'message': 'Ğ Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ½ÑÑ‚Ğ¸Ğ¹',
                'agent_type': 'uninav',
                'agent_name': 'UniNav',
                'confidence': 0.8,
                'response_time': 0.9,
                'language': 'ru'
            }
        ]
        
        # Update user profile with interactions
        for interaction in interactions:
            personalization_engine.update_user_interaction(test_user, interaction)
        
        print("âœ… User interaction tracking tested")
        
        # Test feedback
        personalization_engine.add_user_feedback(test_user, 0.85, "ĞÑ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ!")
        personalization_engine.add_user_feedback(test_user, 0.9, "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾!")
        
        print("âœ… User feedback tested")
        
        # Test agent recommendation
        recommendation = personalization_engine.get_agent_recommendation(
            test_user, 
            "Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğ¸", 
            ['ai_abitur', 'uninav', 'kadrai']
        )
        
        if recommendation:
            agent, confidence = recommendation
            print(f"âœ… Agent Recommendation: {agent} (confidence: {confidence:.2f})")
        
        # Test response adaptation
        base_response = "Ğ”Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ½ÑƒĞ¶Ğ½Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ± Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
        adapted_response = personalization_engine.adapt_response_style(test_user, base_response)
        print(f"âœ… Response Adaptation: '{adapted_response[:50]}...'")
        
        # Test proactive suggestions
        suggestions = personalization_engine.generate_proactive_suggestions(
            test_user, 
            "admission context"
        )
        print(f"âœ… Proactive Suggestions: {len(suggestions)} suggestions generated")
        
        # Get personalization stats
        stats = personalization_engine.get_personalization_stats(test_user)
        print(f"ğŸ“Š User Stats: {stats}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Personalization Engine Test Failed: {e}")
        return False


def test_semantic_search():
    """Test semantic search with knowledge graphs"""
    print("\nğŸ” Testing Semantic Search and Knowledge Graphs...")
    
    try:
        from semantic_search import semantic_search_engine
        
        # Test concept extraction
        test_text = "ĞœĞ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…"
        concepts = semantic_search_engine._extract_concepts(test_text)
        print(f"âœ… Concept Extraction: {concepts}")
        
        # Test semantic similarity
        similarity1 = semantic_search_engine.calculate_semantic_similarity(
            "Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚",
            "Ğ·Ğ°Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ²ÑƒĞ·"
        )
        
        similarity2 = semantic_search_engine.calculate_semantic_similarity(
            "Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ½ÑÑ‚Ğ¸Ğ¹",
            "Ğ¾Ğ±Ñ‰ĞµĞ¶Ğ¸Ñ‚Ğ¸Ğµ"
        )
        
        print(f"âœ… Semantic Similarity: 'Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğµ'â†”'Ğ·Ğ°Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ' = {similarity1:.3f}")
        print(f"âœ… Semantic Similarity: 'Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ'â†”'Ğ¾Ğ±Ñ‰ĞµĞ¶Ğ¸Ñ‚Ğ¸Ğµ' = {similarity2:.3f}")
        
        # Test query expansion
        expanded = semantic_search_engine.expand_query("Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğµ", "medium")
        print(f"âœ… Query Expansion: 'Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğµ' â†’ {expanded}")
        
        # Test concept suggestions
        suggestions = semantic_search_engine.get_concept_suggestions("Ğ¿Ğ¾ÑÑ‚")
        print(f"âœ… Concept Suggestions for 'Ğ¿Ğ¾ÑÑ‚': {suggestions}")
        
        # Test query analysis
        analysis = semantic_search_engine.analyze_query_semantics("ĞšĞ°Ğº Ğ¿Ğ¾Ğ´Ğ°Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğµ?")
        print(f"ğŸ” Query Analysis: {analysis}")
        
        # Get search statistics
        stats = semantic_search_engine.get_search_statistics()
        print(f"ğŸ“Š Search Engine Stats: {stats}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Semantic Search Test Failed: {e}")
        return False


def test_distributed_system():
    """Test distributed caching and async processing"""
    print("\nğŸš€ Testing Distributed System and Performance...")
    
    try:
        from distributed_system import distributed_cache, async_processor, performance_optimizer
        
        # Test distributed cache
        test_key = "test:key:123"
        test_value = {"response": "Test response", "confidence": 0.85}
        
        # Set and get from cache
        set_success = distributed_cache.set(test_key, test_value, 60)
        retrieved_value = distributed_cache.get(test_key)
        
        cache_test_passed = set_success and retrieved_value is not None
        print(f"âœ… Distributed Cache: {'PASS' if cache_test_passed else 'FAIL'}")
        
        # Test async processing
        def test_task(x, y):
            time.sleep(0.1)  # Simulate work
            return x + y
        
        task_id = async_processor.submit_task(test_task, 5, 3)
        result = async_processor.get_result(task_id, timeout=2.0)
        
        async_test_passed = result == 8
        print(f"âœ… Async Processing: {'PASS' if async_test_passed else 'FAIL'}")
        
        # Test performance optimization
        optimization = performance_optimizer.optimize_response_generation(
            "Test message",
            "ai_abitur",
            "ru"
        )
        print(f"âœ… Performance Optimization: {optimization}")
        
        # Get system statistics
        cache_stats = distributed_cache.get_stats()
        processor_stats = async_processor.get_stats()
        optimization_stats = performance_optimizer.get_optimization_stats()
        
        print(f"ğŸ“Š Cache Stats: {cache_stats['combined_stats']}")
        print(f"ğŸ“Š Processor Stats: {processor_stats['statistics']}")
        
        return cache_test_passed and async_test_passed
        
    except Exception as e:
        print(f"âŒ Distributed System Test Failed: {e}")
        return False


def test_agent_integration():
    """Test complete agent integration with all new features"""
    print("\nğŸ¤– Testing Complete Agent Integration...")
    
    try:
        from agents import AgentRouter
        
        router = AgentRouter()
        
        # Test enhanced routing
        test_messages = [
            ("ĞšĞ°Ğº Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚?", "ru", "test_user_1"),
            ("ĞœĞ½Ğµ Ğ½ÑƒĞ¶ĞµĞ½ Ğ¾Ñ‚Ğ¿ÑƒÑĞº", "ru", "test_user_2"),
            ("Ğ Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ¾Ğ²", "ru", "test_user_3"),
            ("ĞŸĞ¾Ğ¼Ğ¾Ğ³Ğ¸Ñ‚Ğµ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ", "ru", "test_user_1"),  # Same user for personalization
            ("ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¶Ğ¸Ñ‚Ğ¸Ğ¸", "ru", "test_user_4")
        ]
        
        results = []
        for message, language, user_id in test_messages:
            print(f"\n  Testing: '{message}' (user: {user_id})")
            
            start_time = time.time()
            result = router.route_message(message, language, user_id)
            response_time = time.time() - start_time
            
            if result and 'response' in result:
                print(f"    âœ… Agent: {result.get('agent_name', 'Unknown')}")
                print(f"    âœ… Confidence: {result.get('confidence', 0):.2f}")
                print(f"    âœ… Response Time: {response_time:.2f}s")
                print(f"    âœ… Cached: {result.get('cached', False)}")
                
                if 'routing_info' in result:
                    routing_info = result['routing_info']
                    print(f"    ğŸ§  ML Scores: {routing_info.get('ml_scores', {})}")
                
                if 'suggestions' in result:
                    print(f"    ğŸ’¡ Suggestions: {len(result['suggestions'])} generated")
                
                results.append(result)
            else:
                print(f"    âŒ No response generated")
        
        # Test feedback system
        if results:
            first_result = results[0]
            router.provide_feedback(
                user_id="test_user_1",
                message="ĞšĞ°Ğº Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚?",
                agent_type=first_result.get('agent_type', ''),
                user_rating=0.9,
                feedback_text="ĞÑ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚!"
            )
            print("âœ… Feedback system tested")
        
        # Get routing analytics
        analytics = router.get_routing_analytics()
        print(f"ğŸ“Š Routing Analytics: {analytics}")
        
        success_rate = len([r for r in results if r.get('confidence', 0) > 0.5]) / len(results)
        print(f"\nğŸ“ˆ Integration Success Rate: {success_rate:.1%}")
        
        return success_rate > 0.8  # 80% success rate threshold
        
    except Exception as e:
        print(f"âŒ Agent Integration Test Failed: {e}")
        return False


def run_comprehensive_test():
    """Run all tests and generate report"""
    print("ğŸš€ Starting Comprehensive Advanced AI Agent Features Test\n")
    print("=" * 70)
    
    test_results = {}
    
    # Run all test modules
    test_functions = [
        ("Intent Classification", test_intent_classification),
        ("Analytics Engine", test_analytics_engine),
        ("Personalization Engine", test_personalization_engine),
        ("Semantic Search", test_semantic_search),
        ("Distributed System", test_distributed_system),
        ("Agent Integration", test_agent_integration)
    ]
    
    passed_tests = 0
    total_tests = len(test_functions)
    
    for test_name, test_func in test_functions:
        print(f"\n{'='*50}")
        try:
            result = test_func()
            test_results[test_name] = result
            if result:
                passed_tests += 1
                status = "âœ… PASSED"
            else:
                status = "âš ï¸ FAILED"
        except Exception as e:
            test_results[test_name] = False
            status = f"âŒ ERROR: {e}"
        
        print(f"\n{test_name}: {status}")
    
    # Generate final report
    print(f"\n{'='*70}")
    print("ğŸ“‹ COMPREHENSIVE TEST REPORT")
    print(f"{'='*70}")
    
    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{test_name:<25} {status}")
    
    success_rate = passed_tests / total_tests * 100
    print(f"\nğŸ“Š Overall Success Rate: {success_rate:.1f}% ({passed_tests}/{total_tests})")
    
    if success_rate >= 80:
        print("ğŸ‰ EXCELLENT: Advanced AI Agent System is working excellently!")
    elif success_rate >= 60:
        print("ğŸ‘ GOOD: Advanced AI Agent System is working well with minor issues")
    else:
        print("âš ï¸ NEEDS ATTENTION: Some advanced features need debugging")
    
    print(f"\nğŸ“ Test completed at {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    return success_rate


if __name__ == "__main__":
    try:
        success_rate = run_comprehensive_test()
        sys.exit(0 if success_rate >= 60 else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Test interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Critical test error: {e}")
        sys.exit(1)